{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a0f8cb1-0655-46d1-a856-1535b793330a",
   "metadata": {},
   "source": [
    "# <center> MPI Programming </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeb1449-c787-4583-9143-8ee3ac547a91",
   "metadata": {},
   "source": [
    "## Programming models\n",
    "*\"Programming for parallel architectures requires that we understand the memory distribution\"*<br>\n",
    "<img src=\"memory_parall.png\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4152432-c9d7-44eb-b8dd-ec48b00a77f4",
   "metadata": {},
   "source": [
    "## Cluster components\n",
    "A typical cluster has the following:\n",
    "- A set of compute nodes, each one with:\n",
    "    - One or more processors\n",
    "    - Memory banks\n",
    "    - Network interfaces\n",
    "- High speed network interconnecting the nodes\n",
    "- A shared filesystem\n",
    "\n",
    "## Distributed communication\n",
    "To communicate 2+ nodes we require\n",
    "- A communication protocol\n",
    "- High speed networks\n",
    "- A shared filesystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b044d10-138d-47c8-9231-efe0ea4a4662",
   "metadata": {},
   "source": [
    "## MPI (Message Passing Interface)\n",
    "### Distributed memory model\n",
    "- A set of tasks that use their own local memory during computation. Multiple tasks can reside on the same physical machine and/or across an arbitrary number of machines.\n",
    "- Tasks exchange data through communications by sending and receiving **messages**.\n",
    "- Data transfer usually requires **cooperative operations** to be performed by each process. For example, a **send** operation must have a matching **receive** operation.\n",
    "\n",
    "<img src=\"messagepass.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c6e05-614f-40f8-bc79-590d748f31f3",
   "metadata": {},
   "source": [
    "### MPI concepts\n",
    "- **MPI is a library, not a language**\n",
    "    - Specifies: names, calling sequences, call to subroutines\n",
    "    - API for C and Fortran\n",
    "    - Link at compile time with MPI library\n",
    "- **MPI is a specification, not a particular implementation.**\n",
    "    - Parallel computing vendors offer an MPI implementation\n",
    "    - Free implementations also available\n",
    "    - Correct MPI code should run on all MPI implementations\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759d059-3f03-4d35-a9ed-caaa805145e9",
   "metadata": {},
   "source": [
    "### Communications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9725f412-6375-47a3-8835-52b9ddf1c69f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "From Wikipedia:<br>\n",
    "    \n",
    "In the message-passing model of parallel computation, the processes executing in parallel have separate address spaces. **Communication** occurs when **a portion of one process’s address space is copied into another process’s address space**. This operation is cooperative and occurs only when the first process executes a send operations and the second process executes. For the **sender**, the obvious arguments that must be specified are the data to be communicated and the destination process to which the data is to be sent. The minimal way to describe data is to specify a starting address and a length (in bytes). Any sort of data item might be used to identify the destination; typically it has been an integer. On the **receiver**’s side, the minimum arguments are the address and length of an area in local memory where the received variable is to be placed, together with a variable to be filled in with the identity of the sender, so that the receiving process can know which process sent it the message.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91f9158-1505-4afe-acca-f94c31cba68d",
   "metadata": {},
   "source": [
    "\n",
    "**Communicator objects** connect groups of processes in the MPI session. Each communicator gives each contained process an independent identifier and arranges its contained processes in an ordered topology. Each communicator has an unique **context** (ID). By default, all processes started by `mpiexec` belong to a communicator called `MPI_COMM_WORLD`.<br>\n",
    "\n",
    "A **Group** is the ordered set of all processes within a communicator. Each process is identified by a **rank**, which in an integer from `0` to `n − 1`, where `n` is the number of processes in the group. Processes can be added or removed from groups, and groups can be operated like sets, using union and intersection operators to create new groups. Once a group is created, we can create a communicator from the group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b6f6f-04ca-45a5-bac5-3c39630e0a0d",
   "metadata": {},
   "source": [
    "### Collective functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a36c4e-8f59-48fe-9a12-d569ebb66a68",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "From Wikipedia:<br>\n",
    "\n",
    "Collective functions involve communication among all processes in a process group (which can mean the entire process pool or a program-defined subset). A typical function is the MPI_Bcast call (short for \"broadcast\"). This function takes data from one node and sends it to all processes in the process group. A reverse operation is the MPI_Reduce call, which takes data from all processes in a group, performs an operation (such as summing), and stores the results on one node. MPI_Reduce is often useful at the start or end of a large distributed calculation, where each processor operates on a part of the data and then combines it into a result.\n",
    "Other operations perform more sophisticated tasks, such as MPI_Alltoall which rearranges n items of data such that the nth node gets the nth item of data from each.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2bfde0-1766-41e0-bb8b-001c65a963a7",
   "metadata": {},
   "source": [
    "### Derived data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98829102-ba18-42f0-bb3b-e2a5109e640c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "From Wikipedia <br>\n",
    "\n",
    "Many MPI functions require that you specify the type of data which is sent between processes. This is because MPI aims to support heterogeneous environments where types might be represented differently on the different nodes(for example they might be running different CPU architectures that have different endianness), in which case MPI implementations can perform data conversion.Since the C language does not allow a type itself to be passed as a parameter, MPI predefines the constants MPI_INT, MPI_CHAR, MPI_DOUBLE to correspond with int, char, double, etc.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e75edf8-ef48-46ba-9513-12f0fa9abe81",
   "metadata": {},
   "source": [
    "## Basic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc1eef3-3568-4626-8483-24b1fc888405",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpicc -o hello_mpi hello_mpi.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e96b8520-7ccf-4b1b-963a-74a0d2ed133d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world from processor pc-familia-at, rank 0 out of 4 processors\n",
      "Hello world from processor pc-familia-at, rank 2 out of 4 processors\n",
      "Hello world from processor pc-familia-at, rank 3 out of 4 processors\n",
      "Hello world from processor pc-familia-at, rank 1 out of 4 processors\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 hello_mpi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
